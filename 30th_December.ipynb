{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#NLP Basics and Tokenization Key Terms\n",
        "\n",
        "##Raw Text and Data Units\n",
        "\n",
        "must watch: https://youtu.be/kCc8FmEb1nY?si=5M1k1KJ_3Q45TYSc\n",
        "\n",
        "Corpus:\n",
        "\n",
        "A large collection of text documents used for training or analysis.\n",
        "\n",
        "Document:\n",
        "\n",
        "A single unit of text such as a sentence, paragraph, or article.\n",
        "\n",
        "Sentence:\n",
        "\n",
        "A sequence of words forming a complete thought, often used as a single input.\n",
        "\n",
        "⸻\n",
        "\n",
        "##Text Preprocessing\n",
        "\n",
        "Text Normalization:\n",
        "\n",
        "The process of cleaning and standardizing text before tokenization.\n",
        "\n",
        "Lowercasing:\n",
        "\n",
        "Converting all text to lowercase to reduce vocabulary size.\n",
        "\n",
        "Punctuation Removal:\n",
        "\n",
        "Removing symbols such as commas and periods when they are not useful.\n",
        "\n",
        "Whitespace Normalization:\n",
        "\n",
        "Removing extra spaces or line breaks.\n",
        "\n",
        "Stop Words:\n",
        "\n",
        "Very common words like “the”, “is”, and “and” that may be removed in classical NLP.\n",
        "\n",
        "⸻\n",
        "\n",
        "##Tokenization Fundamentals\n",
        "\n",
        "Token:\n",
        "\n",
        "The smallest unit of text processed by a language model.\n",
        "\n",
        "Tokenization:\n",
        "\n",
        "The process of splitting raw text into tokens.\n",
        "\n",
        "Tokenizer:\n",
        "\n",
        "The algorithm or tool that converts text into tokens.\n",
        "\n",
        "⸻\n",
        "\n",
        "##Types of Tokenization\n",
        "\n",
        "Word Level Tokenization:\n",
        "\n",
        "Splitting text into words based on spaces or punctuation.\n",
        "\n",
        "Character Level Tokenization:\n",
        "\n",
        "Splitting text into individual characters.\n",
        "\n",
        "Subword Tokenization:\n",
        "\n",
        "Splitting text into units smaller than words but larger than characters.\n",
        "\n",
        "⸻\n",
        "\n",
        "##Modern Tokenization Methods\n",
        "\n",
        "Byte Pair Encoding (BPE):\n",
        "\n",
        "A subword tokenization method that merges frequent character pairs.\n",
        "\n",
        "WordPiece:\n",
        "\n",
        "A subword tokenization method used in BERT that maximizes likelihood.\n",
        "\n",
        "Unigram Language Model:\n",
        "\n",
        "A probabilistic subword tokenization approach.\n",
        "\n",
        "SentencePiece:\n",
        "\n",
        "A tokenizer that treats text as a raw sequence without relying on spaces.\n",
        "\n",
        "⸻\n",
        "\n",
        "##Vocabulary and Encoding\n",
        "\n",
        "Vocabulary:\n",
        "\n",
        "The complete set of tokens known to a tokenizer.\n",
        "\n",
        "Vocabulary Size:\n",
        "\n",
        "The total number of unique tokens in the vocabulary.\n",
        "\n",
        "Token ID:\n",
        "\n",
        "A numerical representation assigned to each token.\n",
        "\n",
        "Encoding:\n",
        "\n",
        "Converting text into token IDs.\n",
        "\n",
        "Decoding:\n",
        "\n",
        "Converting token IDs back into readable text.\n",
        "\n",
        "⸻\n",
        "\n",
        "##Special Tokens\n",
        "\n",
        "Padding Token [PAD]:\n",
        "\n",
        "Used to make all sequences in a batch the same length.\n",
        "\n",
        "Unknown Token [UNK]:\n",
        "\n",
        "Represents words or subwords not present in the vocabulary.\n",
        "\n",
        "Classification Token [CLS]:\n",
        "\n",
        "A special token added at the beginning of a sequence for classification tasks.\n",
        "\n",
        "Separator Token [SEP]:\n",
        "\n",
        "Used to separate sentences or segments in a single input.\n",
        "\n",
        "⸻\n",
        "\n",
        "##Sequence Length Handling\n",
        "\n",
        "Padding:\n",
        "\n",
        "Adding extra tokens to shorter sequences so all sequences have equal length.\n",
        "\n",
        "Truncation:\n",
        "\n",
        "Cutting longer sequences to a fixed maximum length.\n",
        "\n",
        "Maximum Sequence Length:\n",
        "\n",
        "The largest number of tokens a model can process.\n",
        "\n",
        "⸻\n",
        "\n",
        "##Attention Related Concepts\n",
        "\n",
        "Attention Mask:\n",
        "\n",
        "A binary mask indicating which tokens are real and which are padding.\n",
        "\n",
        "Self Attention:\n",
        "\n",
        "A mechanism where each token attends to all other tokens in the sequence.\n",
        "\n",
        "Masked Attention:\n",
        "\n",
        "Preventing certain tokens from participating in attention computation.\n",
        "\n",
        "⸻\n",
        "\n",
        "##Practical NLP Pipeline\n",
        "\n",
        "Tokenize:\n",
        "\n",
        "Convert raw text into tokens.\n",
        "\n",
        "Encode:\n",
        "\n",
        "Convert tokens into token IDs.\n",
        "\n",
        "Pad or Truncate:\n",
        "\n",
        "Adjust sequence lengths for batching.\n",
        "\n",
        "Create Attention Mask:\n",
        "\n",
        "Ensure padding tokens do not affect the model.\n"
      ],
      "metadata": {
        "id": "jWcvHyfinjps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Questions"
      ],
      "metadata": {
        "id": "0j5jsFYIirRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1. Padding and Attention Mask Computation\n",
        "\n",
        "Given batch token lengths:\n",
        "[12, 7, 15, 9]\n",
        "\n",
        "###(a) Longest token size\n",
        "\n",
        "The longest sequence length in the batch is the maximum value in the list.\n",
        "\n",
        "L = max([12, 7, 15, 9]) = 15\n",
        "\n",
        "###(b) Padding added per sequence\n",
        "\n",
        "Padding added to each sequence is calculated as:\n",
        "\n",
        "padding = L - original_length\n",
        "\n",
        "Sequence 1: 15 - 12 = 3\n",
        "Sequence 2: 15 - 7  = 8\n",
        "Sequence 3: 15 - 15 = 0\n",
        "Sequence 4: 15 - 9  = 6\n",
        "\n",
        "Padding per sequence:\n",
        "[3, 8, 0, 6]\n",
        "\n",
        "###(c) Attention mask for the length-7 sequence\n",
        "\n",
        "After padding the length-7 sequence to length 15:\n",
        "\n",
        "- Real tokens are marked with 1\n",
        "- Padding tokens are marked with 0\n",
        "\n",
        "Attention mask:\n",
        "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
      ],
      "metadata": {
        "id": "f7578MgYfR_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#demo code\n",
        "import numpy as np\n",
        "token_lengths = np.array([12, 7, 15, 9])\n",
        "L = max(token_lengths)\n",
        "LM= token_lengths.max()\n",
        "print(\"Longest token size\", LM)\n",
        "\n",
        "#Padding added per sequence\n",
        "Padding = LM - token_lengths\n",
        "print(\"Padding per sequence\", Padding)\n",
        "\n",
        "#Attention mask for the length-7 sequence\n",
        "seq_len=7\n",
        "total_len = 15\n",
        "no_of_zero = total_len - seq_len #15 - 7 = 8\n",
        "attention_mask = np.concatenate([\n",
        "    np.ones(seq_len),\n",
        "    np.zeros(no_of_zero)])\n",
        "print(\"Attention mask for the length-7 sequence\")\n",
        "print(attention_mask.astype(int))"
      ],
      "metadata": {
        "id": "kkP62uA-io3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad34778-6e98-4b19-e70d-340a4d86672d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest token size 15\n",
            "Padding per sequence [3 8 0 6]\n",
            "Attention mask for the length-7 sequence\n",
            "[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.concatenate([np.zeros(5).astype(int),np.ones(5).astype(int)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j516TyHs11n2",
        "outputId": "20cb7a2c-ffbd-493a-f53d-6501de1a3a5a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Given token lengths\n",
        "token_lengths = np.array([12, 7, 15, 9])\n",
        "\n",
        "# (a) Longest token size\n",
        "L = token_lengths.max()\n",
        "\n",
        "# (b) Padding added per sequence\n",
        "padding = L - token_lengths\n",
        "\n",
        "# (c) Attention mask for the length-7 sequence\n",
        "seq_length = 7\n",
        "attention_mask = np.concatenate([\n",
        "    np.ones(seq_length),\n",
        "    np.zeros(L - seq_length)\n",
        "])\n",
        "\n",
        "# Print results\n",
        "print(\"Token lengths:\", token_lengths)\n",
        "print(\"Longest token size (L):\", L)\n",
        "print(\"Padding per sequence:\", padding)\n",
        "print(\"Attention mask for length-7 sequence:\")\n",
        "print(attention_mask.astype(int))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O4f6Ru9fViG",
        "outputId": "8e9d27ca-1bad-4b69-ceb9-8fcf992cd940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token lengths: [12  7 15  9]\n",
            "Longest token size (L): 15\n",
            "Padding per sequence: [3 8 0 6]\n",
            "Attention mask for length-7 sequence:\n",
            "[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2. Truncation with Fixed Maximum Length\n",
        "\n",
        "You choose a fixed maximum token length:\n",
        "Lmax = 128\n",
        "\n",
        "Given tokenized sequence lengths:\n",
        "[80, 140, 128, 200, 50]\n",
        "\n",
        "###(a) Number of truncated samples\n",
        "\n",
        "A sample is truncated if:\n",
        "original_length > Lmax\n",
        "\n",
        "Lengths greater than 128 are:\n",
        "140 and 200\n",
        "\n",
        "Number of truncated samples:\n",
        "2\n",
        "\n",
        "###(b) Tokens removed for each truncated sample\n",
        "\n",
        "Tokens removed are calculated as:\n",
        "removed = original_length - Lmax\n",
        "\n",
        "For length 140:\n",
        "140 - 128 = 12 tokens removed\n",
        "\n",
        "For length 200:\n",
        "200 - 128 = 72 tokens removed\n",
        "\n",
        "###(c) Total tokens removed across the dataset\n",
        "\n",
        "Total removed tokens:\n",
        "12 + 72 = 84 tokens"
      ],
      "metadata": {
        "id": "-gt3AsJ5gYIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#demo code\n",
        "import numpy as np\n",
        "Lmax = 128\n",
        "seq_token= np.array([80, 140, 128, 200, 50])\n",
        "\n",
        "trunkated_mask = seq_token > Lmax\n",
        "print(trunkated_mask)\n",
        "sum = 0\n",
        "for i in trunkated_mask:\n",
        "  if i == True:\n",
        "    sum += 1\n",
        "num_truncated = np.sum(trunkated_mask)\n",
        "print(sum)\n",
        "\n",
        "#(b) Tokens removed for each truncated sample\n",
        "\n",
        "trunkated_seq  = seq_token[trunkated_mask] - Lmax\n",
        "print(trunkated_seq)\n",
        "\n",
        "#(c) Total tokens removed across the dataset\n",
        "print(np.sum(trunkated_seq))"
      ],
      "metadata": {
        "id": "HB-OxhBwinYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c8f4ad-9ac8-4574-edbb-643704955866"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False  True False  True False]\n",
            "2\n",
            "[12 72]\n",
            "84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Fixed maximum length\n",
        "Lmax = 128\n",
        "\n",
        "# Given tokenized lengths\n",
        "token_lengths = np.array([80, 140, 128, 200, 50])\n",
        "\n",
        "# (a) Identify truncated samples\n",
        "truncated_mask = token_lengths > Lmax\n",
        "num_truncated = np.sum(truncated_mask)\n",
        "\n",
        "# (b) Tokens removed per truncated sample\n",
        "removed_tokens = token_lengths[truncated_mask] - Lmax\n",
        "\n",
        "# (c) Total tokens removed\n",
        "total_removed = np.sum(removed_tokens)\n",
        "\n",
        "# Print results\n",
        "print(\"Token lengths:\", token_lengths.tolist())\n",
        "print(\"Fixed maximum length (Lmax):\", Lmax)\n",
        "print(\"Number of truncated samples:\", int(num_truncated))\n",
        "print(\"Tokens removed per truncated sample:\", removed_tokens.tolist())\n",
        "print(\"Total tokens removed:\", int(total_removed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix5t0sw1iSSZ",
        "outputId": "4eefa96e-5780-4549-8898-2db67c38b671"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token lengths: [80, 140, 128, 200, 50]\n",
            "Fixed maximum length (Lmax): 128\n",
            "Number of truncated samples: 2\n",
            "Tokens removed per truncated sample: [12, 72]\n",
            "Total tokens removed: 84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ou55mJglju2k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}